{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8165d79f-b084-4aa6-b4f6-e87267eac66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "This 2023 study analyzes the outcomes of minimally invasive total hip arthroplasty in elderly patients.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import nltk\n",
    "import spacy\n",
    "import string # For punctuation removal later\n",
    "\n",
    "# Load the small English spaCy model we downloaded\n",
    "# We assign it to 'nlp' which is a common convention for the main spaCy object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example sentence relevant to our domain\n",
    "text = \"This 2023 study analyzes the outcomes of minimally invasive total hip arthroplasty in elderly patients.\"\n",
    "\n",
    "# Print the original text\n",
    "print(\"Original Text:\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32eb580c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Tokens:\n",
      "['This', '2023', 'study', 'analyzes', 'the', 'outcomes', 'of', 'minimally', 'invasive', 'total', 'hip', 'arthroplasty', 'in', 'elderly', 'patients', '.']\n",
      "--------------------\n",
      "spaCy Tokens:\n",
      "['This', '2023', 'study', 'analyzes', 'the', 'outcomes', 'of', 'minimally', 'invasive', 'total', 'hip', 'arthroplasty', 'in', 'elderly', 'patients', '.']\n"
     ]
    }
   ],
   "source": [
    "# -----Tokenization Using NLTK-----\n",
    "\n",
    "# Use NLTK recommend word tokenizer \n",
    "nltk_tokens = nltk.word_tokenize(text)\n",
    "\n",
    "print(\"NLTK Tokens:\")\n",
    "print(nltk_tokens)\n",
    "print(\"-\" * 20) # Separator\n",
    "\n",
    "# -----Tokenization Using spaCy-----\n",
    "\n",
    "# Process the text with the loaded spaCy model ('nlp' object)\n",
    "# This creates a 'doc' obj which contains tokens and other annotations\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract the text of each token from the spaCy doc object\n",
    "# Using a list comprehension for conciseness\n",
    "spacy_tokens = [token.text for token in doc]\n",
    "\n",
    "print(\"spaCy Tokens:\")\n",
    "print(spacy_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62f96250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercase Tokens:\n",
      "['this', '2023', 'study', 'analyzes', 'the', 'outcomes', 'of', 'minimally', 'invasive', 'total', 'hip', 'arthroplasty', 'in', 'elderly', 'patients', '.']\n",
      "--------------------\n",
      "Tokens after Stopword Removal:\n",
      "['2023', 'study', 'analyzes', 'outcomes', 'minimally', 'invasive', 'total', 'hip', 'arthroplasty', 'elderly', 'patients', '.']\n"
     ]
    }
   ],
   "source": [
    "# -----Lowercasing-----\n",
    "\n",
    "# Convert tokens from the spaCy list to lowercase\n",
    "# (We'll stick with the spaCy tokens list for subsequent steps,\n",
    "# as spaCy often provides more linguistic features later)\n",
    "lowercase_tokens = [token.lower() for token in spacy_tokens]\n",
    "\n",
    "print(\"Lowercase Tokens:\")\n",
    "print(lowercase_tokens)\n",
    "print(\"-\" * 20) # Separator\n",
    "\n",
    "# -----Stopword Removal-----\n",
    "\n",
    "# Import the list of English stopwords from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) # Use set for faster lookups\n",
    "\n",
    "# You can print the first few stopwords to see what they look like\n",
    "# print(\"Sample Stopwords:\", list(stop_words)[:10])\n",
    "\n",
    "# Remove stopwords from the lowercase list\n",
    "tokens_without_stopwords = [token for token in lowercase_tokens if token not in stop_words]\n",
    "\n",
    "print(\"Tokens after Stopword Removal:\")\n",
    "print(tokens_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "544ae00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens (Meaningful Words):\n",
      "['study', 'analyze', 'outcome', 'minimally', 'invasive', 'total', 'hip', 'arthroplasty', 'elderly', 'patient']\n"
     ]
    }
   ],
   "source": [
    "# --- Lemmatization (using spaCy's pre-processed doc) ---\n",
    "\n",
    "# Remember the 'doc' object we created earlier?\n",
    "# doc = nlp(text)\n",
    "# It contains rich linguistic features, including lemmas.\n",
    "\n",
    "# Let's extract the lemma for each token, BUT only if the token is:\n",
    "# - NOT a stopword (token.is_stop == False)\n",
    "# - NOT punctuation (token.is_punct == False)\n",
    "# - Composed of alphabetic characters (token.is_alpha == True) - this also removes numbers like '2023'\n",
    "\n",
    "lemmatized_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]\n",
    "\n",
    "print(\"Lemmatized Tokens (Meaningful Words):\")\n",
    "print(lemmatized_tokens)\n",
    "\n",
    "# Compare this to the list we had after only removing stopwords:\n",
    "# Previous list: ['2023', 'study', 'analyzes', 'outcomes', 'minimally', 'invasive', 'total', 'hip', 'arthroplasty', 'elderly', 'patients', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02061916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching pubmed for 'total knee arthroplasty' (max 10 results)...\n",
      "Found 10 PMIDs:\n",
      "['40156068', '40155982', '40155353', '40154583', '40153784', '40153477', '40153059', '40151962', '40151740', '40151712']\n"
     ]
    }
   ],
   "source": [
    "# Import the Entrez module from Biopython\n",
    "from Bio import Entrez\n",
    "\n",
    "# --- Configuration ---\n",
    "my_email = \"Your.Name@example.com\" # Replace with your actual email\n",
    "search_query = \"total knee arthroplasty\" # Easily change search term here\n",
    "max_results_search = \"10\" # Easily change number of results here\n",
    "database = \"pubmed\"\n",
    "\n",
    "# --- Set email for NCBI ---\n",
    "Entrez.email = my_email\n",
    "\n",
    "# --- Perform the search (esearch) ---\n",
    "print(f\"Searching {database} for '{search_query}' (max {max_results_search} results)...\")\n",
    "handle = Entrez.esearch(db=database, term=search_query, retmax=max_results_search)\n",
    "search_results = Entrez.read(handle)\n",
    "handle.close()\n",
    "\n",
    "# Extract the list of PubMed IDs (PMIDs)\n",
    "pmid_list = search_results[\"IdList\"]\n",
    "\n",
    "print(f\"Found {len(pmid_list)} PMIDs:\")\n",
    "print(pmid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ca1692e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching details for 10 PMIDs...\n",
      "Successfully parsed 10 articles.\n",
      "\n",
      "Successfully extracted data for 10 papers.\n",
      "\n",
      "--- Sample Extracted Data (First 2 Papers) ---\n",
      "\n",
      "Paper 1:\n",
      "{\n",
      "  \"pmid\": \"40156068\",\n",
      "  \"title\": \"The dual role of titanium particles in osteolysis: implications for gene therapy in prosthesis loosening.\",\n",
      "  \"abstract\": \"Aseptic prosthesis loosening caused by wear particles is a major complication in patients with osteoarthritis following total joint replacement. Despite advancements in treatment, the underlying mechanisms remain poorly understood, and effective therapies are still lacking.\\nIn this study, we investigated the effects of titanium particles on osteoclast and osteoblast differentiation through both in vitro and in vivo experiments.\\nOur findings revealed that titanium particles not only promote the differentiation of RAW264.7 cells into osteoclasts and enhance the secretion of inflammatory factors but also inhibit the differentiation of BMSCs into osteoblasts and reduce the expression of Wnt signaling pathway-related factors. Furthermore, using a mouse model of knee prosthesis loosening and AAV-mediated gene therapy, we demonstrated that the combination of TNF-\\u03b1 interference and Wnt3a overexpression was more effective than single-gene therapy in reducing inflammatory cell infiltration, promoting bone formation, and mitigating bone destruction.\\nThese results highlight the dual role of titanium particles in periprosthetic osteolysis and underscore the potential of a gene therapy strategy targeting both inflammatory factors and the Wnt signaling pathway to improve knee prosthesis loosening. This study provides a scientific foundation for developing novel therapeutic approaches.\",\n",
      "  \"journal\": \"European journal of medical research\",\n",
      "  \"year\": \"2025\"\n",
      "}\n",
      "\n",
      "Paper 2:\n",
      "{\n",
      "  \"pmid\": \"40155982\",\n",
      "  \"title\": \"The comparison of three dimensional and two dimensional evaluation of varus/valgus stress X-rays following total knee arthroplasty.\",\n",
      "  \"abstract\": \"The purpose of this study was to compare three-dimensional (3D) and two-dimensional (2D) evaluation of the stress X-rays following total knee arthroplasty (TKA).\\nThis prospective study analyzed 51 consecutive rTKAs (four males and 44 females, both aged 74\\u2009\\u00b1\\u20096 years). Postoperative varus/valgus stress X-rays were taken at maximum manual varus/valgus stress during knee extension under anesthesia, and were analyzed three-dimensionally using a 3D-2D image matching technique with 3D bone and component models. The 3D models of the femur and tibia, along with component-bone constructs, were reconstructed from CT data using 3D modeling software. The 2D evaluation of varus/valgus stress X-rays were carried out directly on the stress X-rays. The varus/valgus angle (VV angle) between components, Medial joint opening (MJO) and lateral joint opening (LJO) were assessed under conditions of no stress, valgus stress, and varus stress.\\nThe VV angles under no stress, valgus stress, and varus stress in 3D and 2D evaluation were 3.6\\u2009\\u00b1\\u20091.1 / 3.6\\u2009\\u00b1\\u20091.1\\u00b0, -0.6\\u2009\\u00b1\\u20091.6 / -0.6\\u2009\\u00b1\\u20091.6\\u00b0, 7.1\\u2009\\u00b1\\u20091.9 / 6.8\\u2009\\u00b1\\u20092.5\\u00b0, respectively. The MJO in the non-stress condition and under valgus stress in 3D and 2D evaluation were 0.0\\u2009\\u00b1\\u20090.5 / -1.8\\u2009\\u00b1\\u20090.8\\u00a0mm,1.4\\u2009\\u00b1\\u20091.0 / -0.2\\u2009\\u00b1\\u20091.4\\u00a0mm, and the LJO in the non-stress condition and under varus stress in 3D and 2D evaluation were 0.9\\u2009\\u00b1\\u20091.0 / -0.6\\u2009\\u00b1\\u20091.0\\u00a0mm, 3.5\\u2009\\u00b1\\u20091.9 / 2.1\\u2009\\u00b1\\u20091.9\\u00a0mm, respectively.\\nThis prospective study revealed that the 3D evaluation of varus/valgus stress X-rays following total knee arthroplasty is equivalent to 2D evaluation in VV angles, whereas different from 2Devaluation in MJO and LJO.\",\n",
      "  \"journal\": \"Journal of orthopaedic surgery and research\",\n",
      "  \"year\": \"2025\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- Fetch details for the retrieved PMIDs (efetch) ---\n",
    "\n",
    "# List to store the data for all papers\n",
    "papers_data = []\n",
    "\n",
    "if pmid_list:\n",
    "    ids_to_fetch = \",\".join(pmid_list)\n",
    "    print(f\"\\nFetching details for {len(pmid_list)} PMIDs...\")\n",
    "\n",
    "    handle = Entrez.efetch(db=database, id=ids_to_fetch, rettype=\"abstract\", retmode=\"xml\")\n",
    "    try:\n",
    "        # Parse the XML data\n",
    "        papers = Entrez.read(handle)\n",
    "        # The actual list of articles is usually here:\n",
    "        articles = papers.get('PubmedArticle', []) # Use .get for safety\n",
    "    except Exception as e: # Catch broader exceptions during parsing\n",
    "        print(f\"Error parsing XML or reading handle: {e}\")\n",
    "        articles = [] # Ensure articles is an empty list on error\n",
    "    finally:\n",
    "        handle.close()\n",
    "\n",
    "    print(f\"Successfully parsed {len(articles)} articles.\")\n",
    "\n",
    "    # --- Loop through each article and extract data robustly ---\n",
    "    for article_xml in articles:\n",
    "        paper_info = {} # Dictionary to store info for this specific paper\n",
    "        try:\n",
    "            # Use .get() chains to safely access nested dictionary keys\n",
    "            medline_citation = article_xml.get('MedlineCitation', {})\n",
    "            article_details = medline_citation.get('Article', {})\n",
    "            journal_info = article_details.get('Journal', {})\n",
    "            journal_issue = journal_info.get('JournalIssue', {})\n",
    "            pub_date = journal_issue.get('PubDate', {})\n",
    "\n",
    "            # Extract data using .get() with default values ('N/A')\n",
    "            paper_info['pmid'] = medline_citation.get('PMID', 'N/A')\n",
    "            paper_info['title'] = article_details.get('ArticleTitle', 'N/A')\n",
    "\n",
    "            # Abstract extraction needs care - might be list or dict\n",
    "            abstract_section = article_details.get('Abstract', {})\n",
    "            abstract_text_list = abstract_section.get('AbstractText', [])\n",
    "            if isinstance(abstract_text_list, list) and len(abstract_text_list) > 0:\n",
    "                 # Handle cases where abstract is split into sections\n",
    "                 paper_info['abstract'] = \"\\n\".join(map(str, abstract_text_list))\n",
    "            elif isinstance(abstract_text_list, str): # Sometimes it's just a string\n",
    "                 paper_info['abstract'] = abstract_text_list\n",
    "            else:\n",
    "                 paper_info['abstract'] = 'N/A' # Default if no abstract found\n",
    "\n",
    "\n",
    "            paper_info['journal'] = journal_info.get('Title', 'N/A')\n",
    "            # Handle year extraction complexity\n",
    "            year = pub_date.get('Year')\n",
    "            if not year:\n",
    "                 medline_date = pub_date.get('MedlineDate', 'N/A') # Fallback\n",
    "                 # Try to extract year from MedlineDate string (e.g., \"2023 Jan-Feb\")\n",
    "                 import re\n",
    "                 match = re.search(r'\\b(19|20)\\d{2}\\b', medline_date)\n",
    "                 year = match.group(0) if match else 'N/A'\n",
    "            paper_info['year'] = year\n",
    "\n",
    "            # Add other fields if needed (e.g., Authors)\n",
    "\n",
    "            papers_data.append(paper_info) # Add the dictionary to our list\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log error for a specific paper but continue the loop\n",
    "            pmid_for_error = medline_citation.get('PMID', 'UNKNOWN_PMID')\n",
    "            print(f\"Error processing PMID {pmid_for_error}: {e}\")\n",
    "            # Optionally append partial data or skip\n",
    "\n",
    "    print(f\"\\nSuccessfully extracted data for {len(papers_data)} papers.\")\n",
    "\n",
    "else:\n",
    "    print(\"PMID list is empty, skipping fetch.\")\n",
    "\n",
    "\n",
    "# --- Display Data for the First Few Papers Extracted ---\n",
    "if papers_data:\n",
    "    print(\"\\n--- Sample Extracted Data (First 2 Papers) ---\")\n",
    "    # Print the first 2 dictionaries from the list\n",
    "    import json # Use json for pretty printing dictionaries\n",
    "    for i, paper in enumerate(papers_data[:2]): # Loop through the first 2\n",
    "         print(f\"\\nPaper {i+1}:\")\n",
    "         print(json.dumps(paper, indent=2)) # Pretty print the dictionary\n",
    "else:\n",
    "    print(\"No data was extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b00a2658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DataFrame Head (First 5 Rows) ---\n",
      "       pmid                                              title  \\\n",
      "0  40156068  The dual role of titanium particles in osteoly...   \n",
      "1  40155982  The comparison of three dimensional and two di...   \n",
      "2  40155353  The Role of Health Psychology in Surgical Preh...   \n",
      "3  40154583  Comparing Functional Recovery Between Total an...   \n",
      "4  40153784  Convergent and Known-Groups Validity and Sensi...   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  Aseptic prosthesis loosening caused by wear pa...   \n",
      "1  The purpose of this study was to compare three...   \n",
      "2  Approximately 10%-34% of people experience chr...   \n",
      "3  Both total knee arthroplasty (TKA) and unicomp...   \n",
      "4  Subsequent to the COVID-19 pandemic in 2020, a...   \n",
      "\n",
      "                                       journal  year  \n",
      "0         European journal of medical research  2025  \n",
      "1  Journal of orthopaedic surgery and research  2025  \n",
      "2                         Musculoskeletal care  2025  \n",
      "3                  The Journal of arthroplasty  2025  \n",
      "4                      JMIR formative research  2025  \n",
      "\n",
      "--- DataFrame Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   pmid      10 non-null     object\n",
      " 1   title     10 non-null     object\n",
      " 2   abstract  10 non-null     object\n",
      " 3   journal   10 non-null     object\n",
      " 4   year      10 non-null     object\n",
      "dtypes: object(5)\n",
      "memory usage: 532.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# Import the pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the list of dictionaries 'papers_data' into a DataFrame\n",
    "# Pandas handles this conversion directly and efficiently\n",
    "papers_df = pd.DataFrame(papers_data)\n",
    "\n",
    "# Display the first 5 rows of the DataFrame to check\n",
    "print(\"\\n--- DataFrame Head (First 5 Rows) ---\")\n",
    "print(papers_df.head())\n",
    "\n",
    "# Display some basic info about the DataFrame (columns, data types, non-null counts)\n",
    "print(\"\\n--- DataFrame Info ---\")\n",
    "papers_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d049d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a filename for the CSV\n",
    "csv_filename = \"pubmed_sample_tka_10.csv\"\n",
    "\n",
    "# Save the DataFrame to the CSV file\n",
    "# index=False prevents pandas from writing the DataFrame index as a column\n",
    "papers_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"DataFrame with {len(papers_df)} papers saved to: {csv_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
